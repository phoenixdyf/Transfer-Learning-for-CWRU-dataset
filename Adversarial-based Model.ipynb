{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep transfer learning based on GAN**\n",
    "---\n",
    "<font color=black size=3 face=雅黑>**Envirionment**: Tensorflow = 1.14.0 Keras = 2.2.4</font>\n",
    "\n",
    "<font color=black size=3 face=雅黑>**A schematic introduction of implementation proposed adversarial-based models**</font>\n",
    "\n",
    "<font color=black size=2 face=雅黑>In what follows, we explain how to implement a GAN in Keras for DTL-IFD\n",
    "\n",
    "We will train our GAN on 1D raw signal (2D frequency spectrm images) from CWRU dataset, a dataset of 10000 1024x1 monitoring time serie (10000 64x64 RGB images) belong to 10 classes.\n",
    "\n",
    "Schematically, the designed GAN based DTL looks like this:\n",
    "\n",
    "* A `1D_CNN (2D_CNN)` network maps vectors of shape `1024x1(64x64x3)` to extracted feature of shape `(128,1)`.\n",
    "    \n",
    "* A `source_classifier` network maps CNN feature `(128,1)` to a Probability output with ten categories`(10,)`.\n",
    "    \n",
    "* A `domain_classifier` network maps CNN feature `(128,1)` to a binary score estimating the probability that the image is real.\n",
    "\n",
    "*Step 1* : We first train the `source_classifier` to make sure that the `1D_CNN (2D_CNN)` could extract feature for correct classfication task.\n",
    "\n",
    "*Step 2* : We train the `domain_classifier` using examples of source and domain data along with \"real\"/\"fake\" labels,thus the `domain classifier` could tell which sample from source domain (labelled as true) or target domain (labelled as fake)\n",
    "\n",
    "*Step 3* : We train the `1D_CNN (2D_CNN)`, at every step, we move the weights of `1D_CNN (2D_CNN)` in a direction that will make the discriminator more likely to classify the samples from target domain as \"true\" . I.e. we train the generator to fool the discriminator.\n",
    "\n",
    "**By iteratively conducting step 1 to step 3, the domain shift could be covered and the classfication task from source domain to target domain could be implemented** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation,Reshape, Flatten, Conv2D,Conv1D,BatchNormalization, MaxPooling2D,PReLU, Dropout,Conv1D,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load 1D data\n",
    "X_0_1D = np.load(\"./DE/X_0.npy\").reshape((-1,1024,1))\n",
    "X_1_1D = np.load(\"./DE/X_1.npy\").reshape((-1,1024,1))\n",
    "X_2_1D = np.load(\"./DE/X_2.npy\").reshape((-1,1024,1))\n",
    "X_3_1D = np.load(\"./DE/X_3.npy\").reshape((-1,1024,1))\n",
    "# Load 2D data\n",
    "X_0_2D = np.load(\"./DE/Case1_FFT.npy\").reshape((-1,64,64,3))/255-0.5\n",
    "X_1_2D = np.load(\"./DE/Case2_FFT.npy\").reshape((-1,64,64,3))/255-0.5\n",
    "X_2_2D = np.load(\"./DE/Case3_FFT.npy\").reshape((-1,64,64,3))/255-0.5\n",
    "X_3_2D = np.load(\"./DE/Case4_FFT.npy\").reshape((-1,64,64,3))/255-0.5\n",
    "Y_0 = np.load(\"./DE/Y_0.npy\")\n",
    "Y_1 = np.load(\"./DE/Y_1.npy\")\n",
    "Y_2 = np.load(\"./DE/Y_2.npy\")\n",
    "Y_3 = np.load(\"./DE/Y_3.npy\")\n",
    "# define the function to output shuffled dataset \n",
    "def Shuffle(x,y,split):\n",
    "    index=np.random.choice(a=len(x), size=len(x), replace=False, p=None)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in range(len(x)):\n",
    "        slice_x = x[index[i]]\n",
    "        slice_y = y[index[i]]\n",
    "        X.append(slice_x)\n",
    "        Y.append(slice_y)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Y = to_categorical(Y)\n",
    "    train_X = X[0:int(len(x)*split)]\n",
    "    train_Y = Y[0:int(len(x)*split)]\n",
    "    test_X = X[int(len(x)*split):]\n",
    "    test_Y = Y[int(len(x)*split):]\n",
    "    return train_X,train_Y,test_X,test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 GAN model construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 1D-GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GAN_1D():\n",
    "    \"\"\"Creates three different models, one used for source only training, two used for domain adaptation\"\"\"\n",
    "    inputs = Input(shape=(1024,1)) \n",
    "    x4 = Conv1D(128,3)(inputs)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv1D(64,3)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv1D(32,3)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    x4 = Dense(128)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Reshape((128,1))(x4)\n",
    "\n",
    "\n",
    "    source_classifier = Conv1D(32,3, name=\"mo1\")(x4)\n",
    "    source_classifier = LeakyReLU(name=\"mo2\")(source_classifier)\n",
    "    source_classifier = Dropout(0.25)(source_classifier)\n",
    "    source_classifier = Conv1D(16,3, name=\"mo3\")(source_classifier)\n",
    "    source_classifier = LeakyReLU(name=\"mo4\")(source_classifier)\n",
    "    source_classifier = Flatten()(source_classifier)\n",
    "    source_classifier = Dense(64,name=\"mo5\")(source_classifier)\n",
    "    source_classifier = LeakyReLU(name=\"mo6\")(source_classifier)\n",
    "    source_classifier = Dense(10, activation='softmax', name=\"mo\")(source_classifier)  \n",
    "    \n",
    "\n",
    "    domain_classifier = Conv1D(256,3, name=\"do1\")(x4)\n",
    "    domain_classifier = LeakyReLU(name=\"do2\")(domain_classifier)\n",
    "    domain_classifier = Dropout(0.25)(domain_classifier)\n",
    "    domain_classifier = Conv1D(128,3, name=\"do3\")(domain_classifier)\n",
    "    domain_classifier = LeakyReLU(name=\"do4\")(domain_classifier)\n",
    "    domain_classifier = Dropout(0.25)(domain_classifier)\n",
    "    domain_classifier = Flatten()(domain_classifier)\n",
    "    domain_classifier = Dense(512, name=\"do5\")(domain_classifier)\n",
    "    domain_classifier = LeakyReLU(name=\"do6\")(domain_classifier)\n",
    "    domain_classifier = Dense(128, name=\"do7\")(domain_classifier)\n",
    "    domain_classifier = LeakyReLU(name=\"do8\")(domain_classifier)\n",
    "    domain_classifier = Dense(2, activation='softmax', name=\"do\")(domain_classifier)\n",
    "\n",
    "    comb_model = Model(inputs=inputs, outputs=[source_classifier, domain_classifier])\n",
    "    comb_model.compile(optimizer=\"Adam\",\n",
    "              loss={'mo': 'categorical_crossentropy', 'do': 'binary_crossentropy'},\n",
    "              loss_weights={'mo': 2, 'do': 1}, metrics=['accuracy'], )\n",
    "\n",
    "    source_classification_model = Model(inputs=inputs, outputs=[source_classifier])\n",
    "    source_classification_model.compile(optimizer=\"Adam\",\n",
    "              loss={'mo': 'categorical_crossentropy'}, metrics=['accuracy'], )\n",
    "\n",
    "    domain_classification_model = Model(inputs=inputs, outputs=[domain_classifier])\n",
    "    domain_classification_model.compile(optimizer=\"Adam\",\n",
    "                  loss={'do': 'binary_crossentropy'}, metrics=['accuracy'])\n",
    "    \n",
    "    embeddings_model = Model(inputs=inputs, outputs=[x4])\n",
    "    embeddings_model.compile(optimizer=\"Adam\",loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "                        \n",
    "                        \n",
    "    return comb_model, source_classification_model, domain_classification_model, embeddings_model\n",
    "def train_1D(trainX, trainY, trainDX, trainDY, testX,testY,testDX,testDY, save_model_name,enable_GAN = True, n_iterations =1500):\n",
    "    \n",
    "    batch_size =64\n",
    "    \n",
    "    model, source_classification_model, domain_classification_model, embeddings_model = GAN_1D()\n",
    "\n",
    "    y_class_dummy = np.ones((len(trainX), 2))\n",
    "    y_adversarial_1 = to_categorical(np.array(([1] * batch_size + [0] * batch_size)))\n",
    "    \n",
    "    sample_weights_class = np.array(([1] * batch_size + [0] * batch_size))\n",
    "    sample_weights_adversarial = np.ones((batch_size * 2,))\n",
    "\n",
    "    start = 0\n",
    "    temp = 0\n",
    "    for i in range(n_iterations):\n",
    "        stop = start + batch_size\n",
    "        # # print(y_class_dummy.shape, ys.shape)\n",
    "        y_adversarial_2 = to_categorical(np.array(([0] * batch_size + [1] * batch_size)))\n",
    "        X0 = trainX[start: stop]\n",
    "        X1 = trainDX[start: stop]\n",
    "        y0 = trainY[start: stop]\n",
    "        y1 = np.zeros(shape = (len(trainX),2))[start: stop]\n",
    "\n",
    "\n",
    "        X_adv = np.concatenate([X0, X1])\n",
    "        y_class = np.concatenate([y0, np.zeros_like(y0)])\n",
    "\n",
    "        adv_weights = []\n",
    "        for layer in model.layers:\n",
    "            if (layer.name.startswith(\"do\")):\n",
    "                adv_weights.append(layer.get_weights())\n",
    "\n",
    "        if(enable_GAN):\n",
    "            # note - even though we save and append weights, the batchnorms moving means and variances\n",
    "            # are not saved throught this mechanism \n",
    "            stats = model.train_on_batch(X_adv, [y_class, y_adversarial_1],\n",
    "                                     sample_weight=[sample_weights_class, sample_weights_adversarial])\n",
    "            ## 这里训练是用源域的标签先训练分类器，同时也训练了域分类器\n",
    "            k = 0\n",
    "            for layer in model.layers:\n",
    "                if (layer.name.startswith(\"do\")):\n",
    "                    layer.set_weights(adv_weights[k])\n",
    "                    k += 1\n",
    "            ## 这里说的是将之前训练的域分类器的权重给到域分类器中\n",
    "            class_weights = []\n",
    "            \n",
    "        \n",
    "            for layer in model.layers:\n",
    "                if (not layer.name.startswith(\"do\")):\n",
    "                    class_weights.append(layer.get_weights())\n",
    "            ## 这里说的是将之前训练的标签分类器的权重给到源域分类器中\n",
    "            ## 这步相当于说是在训练真实的域分类器，更新DO层的权重\n",
    "            stats2 = domain_classification_model.train_on_batch(X_adv, [y_adversarial_2])\n",
    "\n",
    "            k = 0\n",
    "            for layer in model.layers:\n",
    "                if (not layer.name.startswith(\"do\")):\n",
    "                    layer.set_weights(class_weights[k])\n",
    "                    k += 1\n",
    "\n",
    "        else:\n",
    "            source_classification_model.train_on_batch(X0,y0)\n",
    "        start += batch_size\n",
    "        if start > len(trainX) - batch_size:\n",
    "            start = 0\n",
    "\n",
    "        #if ((i + 1) % 1 == 0):\n",
    "            #y_test_hat_t = source_classification_model.predict(trainDX).argmax(1)\n",
    "            #if accuracy_score(trainDY.argmax(1), y_test_hat_t)>0.95:\n",
    "                #print(\"Found a good result at Iteration %d,target accuracy = %.3f\"%(i,accuracy_score(trainDY.argmax(1), y_test_hat_t)))\n",
    "                #source_classification_model.save(\"./models/model at Iteration %d with accuracy %.3f\"%(i,accuracy_score(trainDY.argmax(1), y_test_hat_t)))\n",
    "        if ((i + 1) % 25 == 0):\n",
    "            # print(i, stats)\n",
    "            y_test_hat_t = source_classification_model.predict(testDX).argmax(1)\n",
    "            y_test_hat_s = source_classification_model.predict(testX).argmax(1)\n",
    "            print(\"Iteration %d, source accuracy =  %.3f, target accuracy = %.3f\"%(i, accuracy_score(testY.argmax(1), y_test_hat_s), accuracy_score(testDY.argmax(1), y_test_hat_t)))\n",
    "        \n",
    "        if ((i + 1) % 25 == 0):            \n",
    "            y_test_hat = source_classification_model.predict(testDX).argmax(1)\n",
    "            threshold = accuracy_score(testDY.argmax(1), y_test_hat)\n",
    "            if threshold > temp:\n",
    "                # Save the model and update it when testing acc has improvement\n",
    "                temp = threshold\n",
    "                print(\"save the model at Iteration %d with target accuracy = %.3f\"%(i,threshold))\n",
    "                source_classification_model.save(\"./models/\"+save_model_name+\".h5\")\n",
    "    return embeddings_model,source_classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 2D-GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GAN_2D():\n",
    "    inputs = Input(shape=(64,64,3)) \n",
    "    x4 = Conv2D(128,(3,3))(inputs)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv2D(64,(3,3))(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv2D(32,(3,3))(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    x4 = Dense(128)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Reshape((128,1))(x4)\n",
    "    source_classifier = Conv1D(32,3, name=\"mo1\")(x4)\n",
    "    source_classifier = LeakyReLU(name=\"mo2\")(source_classifier)\n",
    "    source_classifier = Dropout(0.25)(source_classifier)\n",
    "    source_classifier = Conv1D(16,3, name=\"mo3\")(source_classifier)\n",
    "    source_classifier = LeakyReLU(name=\"mo4\")(source_classifier)\n",
    "    source_classifier = Flatten()(source_classifier)\n",
    "    source_classifier = Dense(64,name=\"mo5\")(source_classifier)\n",
    "    source_classifier = LeakyReLU(name=\"mo6\")(source_classifier)\n",
    "    source_classifier = Dense(10, activation='softmax', name=\"mo\")(source_classifier)  \n",
    "    \n",
    "\n",
    "    domain_classifier = Conv1D(256,3, name=\"do1\")(x4)\n",
    "    domain_classifier = LeakyReLU(name=\"do2\")(domain_classifier)\n",
    "    domain_classifier = Dropout(0.25)(domain_classifier)\n",
    "    domain_classifier = Conv1D(128,3, name=\"do3\")(domain_classifier)\n",
    "    domain_classifier = LeakyReLU(name=\"do4\")(domain_classifier)\n",
    "    domain_classifier = Dropout(0.25)(domain_classifier)\n",
    "    domain_classifier = Flatten()(domain_classifier)\n",
    "    domain_classifier = Dense(512, name=\"do5\")(domain_classifier)\n",
    "    domain_classifier = LeakyReLU(name=\"do6\")(domain_classifier)\n",
    "    domain_classifier = Dense(128, name=\"do7\")(domain_classifier)\n",
    "    domain_classifier = LeakyReLU(name=\"do8\")(domain_classifier)\n",
    "    domain_classifier = Dense(2, activation='softmax', name=\"do\")(domain_classifier)\n",
    "\n",
    "    comb_model = Model(inputs=inputs, outputs=[source_classifier, domain_classifier])\n",
    "    comb_model.compile(optimizer=\"Adam\",\n",
    "              loss={'mo': 'categorical_crossentropy', 'do': 'binary_crossentropy'},\n",
    "              loss_weights={'mo': 2, 'do': 1}, metrics=['accuracy'], )\n",
    "\n",
    "    source_classification_model = Model(inputs=inputs, outputs=[source_classifier])\n",
    "    source_classification_model.compile(optimizer=\"Adam\",\n",
    "              loss={'mo': 'categorical_crossentropy'}, metrics=['accuracy'], )\n",
    "\n",
    "    domain_classification_model = Model(inputs=inputs, outputs=[domain_classifier])\n",
    "    domain_classification_model.compile(optimizer=\"Adam\",\n",
    "                  loss={'do': 'binary_crossentropy'}, metrics=['accuracy'])\n",
    "    \n",
    "    embeddings_model = Model(inputs=inputs, outputs=[x4])\n",
    "    embeddings_model.compile(optimizer=\"Adam\",loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "                        \n",
    "                        \n",
    "    return comb_model, source_classification_model, domain_classification_model, embeddings_model\n",
    "def train_2D(trainX, trainY, trainDX, trainDY, testX,testY,testDX,testDY, save_model_name,enable_GAN = True, n_iterations =1000):\n",
    "    \n",
    "    batch_size =64\n",
    "    \n",
    "    model, source_classification_model, domain_classification_model, embeddings_model = GAN_2D()\n",
    "\n",
    "    y_class_dummy = np.ones((len(trainX), 2))\n",
    "    y_adversarial_1 = to_categorical(np.array(([1] * batch_size + [0] * batch_size)))\n",
    "    \n",
    "    sample_weights_class = np.array(([1] * batch_size + [0] * batch_size))\n",
    "    sample_weights_adversarial = np.ones((batch_size * 2,))\n",
    "\n",
    "    start = 0\n",
    "    temp = 0\n",
    "    for i in range(n_iterations):\n",
    "        stop = start + batch_size\n",
    "        # # print(y_class_dummy.shape, ys.shape)\n",
    "        y_adversarial_2 = to_categorical(np.array(([0] * batch_size + [1] * batch_size)))\n",
    "        X0 = trainX[start: stop]\n",
    "        X1 = trainDX[start: stop]\n",
    "        y0 = trainY[start: stop]\n",
    "        y1 = np.zeros(shape = (len(trainX),2))[start: stop]\n",
    "\n",
    "\n",
    "        X_adv = np.concatenate([X0, X1])\n",
    "        y_class = np.concatenate([y0, np.zeros_like(y0)])\n",
    "\n",
    "        adv_weights = []\n",
    "        for layer in model.layers:\n",
    "            if (layer.name.startswith(\"do\")):\n",
    "                adv_weights.append(layer.get_weights())\n",
    "\n",
    "        if(enable_GAN):\n",
    "            # note - even though we save and append weights, the batchnorms moving means and variances\n",
    "            # are not saved throught this mechanism \n",
    "            stats = model.train_on_batch(X_adv, [y_class, y_adversarial_1],\n",
    "                                     sample_weight=[sample_weights_class, sample_weights_adversarial])\n",
    "            ## 这里训练是用源域的标签先训练分类器，同时也训练了域分类器\n",
    "            k = 0\n",
    "            for layer in model.layers:\n",
    "                if (layer.name.startswith(\"do\")):\n",
    "                    layer.set_weights(adv_weights[k])\n",
    "                    k += 1\n",
    "            ## 这里说的是将之前训练的域分类器的权重给到域分类器中\n",
    "            class_weights = []\n",
    "            \n",
    "        \n",
    "            for layer in model.layers:\n",
    "                if (not layer.name.startswith(\"do\")):\n",
    "                    class_weights.append(layer.get_weights())\n",
    "            ## 这里说的是将之前训练的标签分类器的权重给到源域分类器中\n",
    "            ## 这步相当于说是在训练真实的域分类器，更新DO层的权重\n",
    "            stats2 = domain_classification_model.train_on_batch(X_adv, [y_adversarial_2])\n",
    "\n",
    "            k = 0\n",
    "            for layer in model.layers:\n",
    "                if (not layer.name.startswith(\"do\")):\n",
    "                    layer.set_weights(class_weights[k])\n",
    "                    k += 1\n",
    "\n",
    "        else:\n",
    "            source_classification_model.train_on_batch(X0,y0)\n",
    "        start += batch_size\n",
    "        if start > len(trainX) - batch_size:\n",
    "            start = 0\n",
    "        # check the model performance each 100 iteration\n",
    "        \n",
    "        if ((i + 1) % 100 == 0):            \n",
    "            y_test_hat = source_classification_model.predict(testDX).argmax(1)\n",
    "            threshold = accuracy_score(testDY.argmax(1), y_test_hat)\n",
    "            if threshold > temp:\n",
    "                # Save the model and update it when testing acc has improvement\n",
    "                temp = threshold\n",
    "                print(\"save the model at Iteration %d with target accuracy = %.3f\"%(i,threshold))\n",
    "                source_classification_model.save(save_model_name+\".h5\")\n",
    "                if i >= 1000:\n",
    "                    break\n",
    "            \n",
    "    return embeddings_model,source_classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "save the model at Iteration 99 with target accuracy = 0.957\n",
      "save the model at Iteration 299 with target accuracy = 0.994\n",
      "save the model at Iteration 499 with target accuracy = 0.995\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY,testX,testY = Shuffle(X_0_2D,Y_0,split=0.8)\n",
    "trainDX,trainDY,testDX,testDY = Shuffle(X_1_2D,Y_1,split=0.8)\n",
    "save_model_name=\"Adversarial_Task1_2D\"\n",
    "gan_model,classifier = train_2D(trainX,trainY,trainDX,trainDY,testX,testY,testDX,testDY,save_model_name,enable_GAN = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Model testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 2ms/step\n",
      "200/200 [==============================] - 0s 304us/step\n",
      "200/200 [==============================] - 0s 312us/step\n",
      "200/200 [==============================] - 0s 306us/step\n",
      "200/200 [==============================] - 0s 293us/step\n",
      "200/200 [==============================] - 0s 290us/step\n",
      "200/200 [==============================] - 0s 289us/step\n",
      "200/200 [==============================] - 0s 288us/step\n",
      "200/200 [==============================] - 0s 294us/step\n",
      "200/200 [==============================] - ETA:  - 0s 286us/step\n",
      "Mean acc: 99.45% Prediction variance: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "model=load_model(save_model_name+\".h5\")\n",
    "\n",
    "def test_model(model,testDX,testDY):\n",
    "    acc=np.zeros(10,)\n",
    "    for i in range(10):\n",
    "        acc[i]=model.evaluate(testDX[0+(len(testDX)//10)*i:(len(testDX)//10)+(len(testDX)//10)*i],testDY[0+(len(testDX)//10)*i:(len(testDX)//10)+(len(testDX)//10)*i])[1]\n",
    "    print(\"Mean acc: %.2f\"%(np.mean(acc)*100)+\"%\",\"Prediction variance: %.2f\"%(np.var(acc)*100)+\"%\")\n",
    "\n",
    "test_model(model,testDX,testDY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.458px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
