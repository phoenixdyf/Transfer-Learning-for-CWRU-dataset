{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep transfer learning based on fine-tuning**\n",
    "---\n",
    "<font color=black size=3 face=雅黑>**Envirionment**: Tensorflow = 1.14.0 Keras = 2.2.4</font>\n",
    "\n",
    "<font color=black size=3 face=雅黑>**A schematic introduction of implementation proposed parameter-based models**</font>\n",
    "\n",
    "<font color=black size=2 face=雅黑>In what follows, we explain how to implement a fine-tune (parameter-based model) in Keras for DTL-IFD\n",
    "\n",
    "Schematically, the designed parameter-based DTL looks like this:\n",
    "\n",
    "* A `Base_Net` model based on 1D_CNN (2D_CNN) maps vectors of shape `1024x1(64x64x3)` to extracted feature of shape `(128,1)`.\n",
    "    \n",
    "* A `Finetune_Net` model maps CNN feature `(128,1)` to a Probability output with ten categories`(10,)`.\n",
    "    \n",
    "*Step 1* : We first use the source data to train `Base_Net` and `Finetune_Net`, making sure that these two models could correctly classify 10 categories of source data\n",
    "\n",
    "*Step 2* : We **freeze** the trained model parameters of  `Base_Net`, use this parameter for target data\n",
    "\n",
    "*Step 3* : We import the target data into the model and only optimize the parameters of `Finetune_Net`.\n",
    "\n",
    "**By iteratively conducting step 1 to step 3, the domain shift could be covered and the classfication task from source domain to target domain could be implemented** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the basic modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Activation,Reshape, Flatten, Conv2D,Conv1D,BatchNormalization, MaxPooling2D,PReLU, Dropout,Conv1D,GlobalAveragePooling1D\n",
    "from keras import models\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 1D data\n",
    "X_0_1D = np.load(\"./DE/X_0.npy\").reshape((-1,1024,1))\n",
    "X_1_1D = np.load(\"./DE/X_1.npy\").reshape((-1,1024,1))\n",
    "X_2_1D = np.load(\"./DE/X_2.npy\").reshape((-1,1024,1))\n",
    "X_3_1D = np.load(\"./DE/X_3.npy\").reshape((-1,1024,1))\n",
    "# Load 2D data\n",
    "X_0_2D = np.load(\"./DE/Case1_FFT.npy\").reshape((-1,64,64,3))/255\n",
    "X_1_2D = np.load(\"./DE/Case2_FFT.npy\").reshape((-1,64,64,3))/255\n",
    "X_2_2D = np.load(\"./DE/Case3_FFT.npy\").reshape((-1,64,64,3))/255\n",
    "X_3_2D = np.load(\"./DE/Case4_FFT.npy\").reshape((-1,64,64,3))/255\n",
    "Y_0 = np.load(\"./DE/Y_0.npy\")\n",
    "Y_1 = np.load(\"./DE/Y_1.npy\")\n",
    "Y_2 = np.load(\"./DE/Y_2.npy\")\n",
    "Y_3 = np.load(\"./DE/Y_3.npy\")\n",
    "def Shuffle(x,y,split):\n",
    "    index=np.random.choice(a=len(x), size=len(x), replace=False, p=None)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for i in range(len(x)):\n",
    "        slice_x = x[index[i]]\n",
    "        slice_y = y[index[i]]\n",
    "        X.append(slice_x)\n",
    "        Y.append(slice_y)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Y = to_categorical(Y)\n",
    "    train_X = X[0:int(len(x)*split)]\n",
    "    train_Y = Y[0:int(len(x)*split)]\n",
    "    test_X = X[int(len(x)*split):]\n",
    "    test_Y = Y[int(len(x)*split):]\n",
    "    return train_X,train_Y,test_X,test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defination of feature-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defination of Base_Net for 1D-data\n",
    "def CNN_1D():\n",
    "    \"\"\"Creates three different models, one used for source only training, two used for domain adaptation\"\"\"\n",
    "    inputs = Input(shape=(1024,1)) \n",
    "    x4 = Conv1D(128,3)(inputs)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv1D(64,3)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv1D(32,3)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    x4 = Dense(128)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Reshape((128,1))(x4)\n",
    "    model = Model(inputs=inputs, outputs=x4)\n",
    "    return model\n",
    "# defination of Base_Net for 2D-data\n",
    "def CNN_2D():\n",
    "    \"\"\"Creates three different models, one used for source only training, two used for domain adaptation\"\"\"\n",
    "    inputs = Input(shape=(64,64,3)) \n",
    "    x4 = Conv2D(128,(3,3))(inputs)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv2D(64,(3,3))(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv2D(32,(3,3))(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    x4 = Dense(128)(x4)\n",
    "    x4 = LeakyReLU()(x4)\n",
    "    x4 = Reshape((128,1))(x4)\n",
    "    model = Model(inputs=inputs, outputs=x4)\n",
    "    return model\n",
    "# defination of Finetune_net\n",
    "def Classify():\n",
    "    inputs = Input(shape=(128,1)) \n",
    "    x4= Conv1D(32,3, name=\"mo1\")(inputs)\n",
    "    x4 = LeakyReLU(name=\"mo2\")(x4)\n",
    "    x4 = Dropout(0.25)(x4)\n",
    "    x4 = Conv1D(16,3, name=\"mo3\")(x4)\n",
    "    x4 = LeakyReLU(name=\"mo4\")(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    x4 = Dense(64,name=\"mo5\")(x4)\n",
    "    x4 = LeakyReLU(name=\"mo6\")(x4)\n",
    "    x4 = Dense(10, activation='softmax', name=\"mo\")(x4)  \n",
    "    model = Model(inputs=inputs, outputs=x4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_net_1D = CNN_1D()\n",
    "base_net_2D = CNN_2D()\n",
    "finetune_net = Classify()\n",
    "\n",
    "model_1D = models.Sequential()\n",
    "model_1D.add(base_net_1D)\n",
    "model_1D.add(finetune_net)\n",
    "model_1D.compile(optimizer=\"Adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_2D = models.Sequential()\n",
    "model_2D.add(base_net_2D)\n",
    "model_2D.add(finetune_net)\n",
    "model_2D.compile(optimizer=\"Adam\",loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 8s 988us/step - loss: 0.8319 - acc: 0.6749 - val_loss: 0.2585 - val_acc: 0.9250\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 0.2219 - acc: 0.9289 - val_loss: 0.1567 - val_acc: 0.9370\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.1138 - acc: 0.9584 - val_loss: 0.0599 - val_acc: 0.9790\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.0376 - acc: 0.9873 - val_loss: 0.2556 - val_acc: 0.9235\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 0.0113 - acc: 0.9964 - val_loss: 0.0740 - val_acc: 0.9720\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 0.0442 - acc: 0.9850 - val_loss: 0.0706 - val_acc: 0.9725\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.0662 - acc: 0.9822 - val_loss: 0.0592 - val_acc: 0.9740\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.0236 - acc: 0.9921 - val_loss: 0.0163 - val_acc: 0.9940\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.0173 - acc: 0.9936 - val_loss: 0.0806 - val_acc: 0.9720\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 0.0144 - acc: 0.9958 - val_loss: 0.0324 - val_acc: 0.9880\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.0348 - acc: 0.9882 - val_loss: 0.1121 - val_acc: 0.9665\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.0120 - val_acc: 0.9960\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 2.9018e-04 - acc: 1.0000 - val_loss: 0.0229 - val_acc: 0.9915\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 1.6143e-04 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9945\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 1.1873e-04 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 0.9945\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 2.0529e-04 - acc: 1.0000 - val_loss: 0.0161 - val_acc: 0.9935\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 8.1532e-05 - acc: 1.0000 - val_loss: 0.0115 - val_acc: 0.9950\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 5.7350e-05 - acc: 1.0000 - val_loss: 0.0145 - val_acc: 0.9935\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 7.2765e-05 - acc: 1.0000 - val_loss: 0.0190 - val_acc: 0.9930\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 4.6703e-05 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9945\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 3.2806e-05 - acc: 1.0000 - val_loss: 0.0145 - val_acc: 0.9935\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 3.6274e-05 - acc: 1.0000 - val_loss: 0.0115 - val_acc: 0.9945\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 3.6212e-05 - acc: 1.0000 - val_loss: 0.0124 - val_acc: 0.9950\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 2.8223e-05 - acc: 1.0000 - val_loss: 0.0148 - val_acc: 0.9945\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 4.8231e-05 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9950\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 3.2495e-05 - acc: 1.0000 - val_loss: 0.0091 - val_acc: 0.9950\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 2.2225e-05 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 0.9945\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 1.4993e-05 - acc: 1.0000 - val_loss: 0.0114 - val_acc: 0.9950\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 2.6612e-05 - acc: 1.0000 - val_loss: 0.0094 - val_acc: 0.9955\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 2.4508e-05 - acc: 1.0000 - val_loss: 0.0136 - val_acc: 0.9945\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 2.0830e-05 - acc: 1.0000 - val_loss: 0.0171 - val_acc: 0.9940\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 2s 272us/step - loss: 9.2344e-06 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9945\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 1.2785e-05 - acc: 1.0000 - val_loss: 0.0106 - val_acc: 0.9955\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 2.7504e-05 - acc: 1.0000 - val_loss: 0.0151 - val_acc: 0.9940\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 8.6001e-06 - acc: 1.0000 - val_loss: 0.0130 - val_acc: 0.9950\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 8.7758e-06 - acc: 1.0000 - val_loss: 0.0097 - val_acc: 0.9955\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 1.6197e-05 - acc: 1.0000 - val_loss: 0.0196 - val_acc: 0.9935\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 9.6804e-06 - acc: 1.000 - 2s 277us/step - loss: 9.5330e-06 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 0.9945\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 9.2235e-06 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 0.9970\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 8.4979e-06 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9935\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 2.0071e-05 - acc: 1.0000 - val_loss: 0.0189 - val_acc: 0.9930\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 2.7821e-06 - acc: 1.0000 - val_loss: 0.0117 - val_acc: 0.9950\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 4.6944e-06 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9945\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 4.2092e-06 - acc: 1.0000 - val_loss: 0.0096 - val_acc: 0.9955\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 2.9353e-06 - acc: 1.0000 - val_loss: 0.0120 - val_acc: 0.9950\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 3.6320e-06 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9955\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 1.7074e-05 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9970\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 2s 278us/step - loss: 7.9624e-06 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 0.9975\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 5.2364e-06 - acc: 1.0000 - val_loss: 0.0076 - val_acc: 0.9970\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 3.7700e-06 - acc: 1.0000 - val_loss: 0.0070 - val_acc: 0.9975\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 10s 1ms/step - loss: 14.4707 - acc: 0.0974 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 8s 1ms/step - loss: 14.5426 - acc: 0.0977 - val_loss: 14.3612 - val_acc: 0.1090\n"
     ]
    }
   ],
   "source": [
    "trainX,trainY,testX,testY = Shuffle(X_0_1D,Y_0,split=0.8)\n",
    "model_1D.fit(trainX,trainY,batch_size=64,epochs=50,validation_data=[testX,testY])\n",
    "model_1D.save_weights(\"pre-trained-1D.h5\")\n",
    "trainX,trainY,testX,testY = Shuffle(X_0_2D,Y_0,split=0.8)\n",
    "model_2D.fit(trainX,trainY,batch_size=64,epochs=50,validation_data=[testX,testY])\n",
    "model_2D.save_weights(\"pre-trained-2D.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_1D.load_weights('pre-trained-1D.h5', by_name=True)\n",
    "base_net_1D.trainable=False\n",
    "model_2D.load_weights('pre-trained-2D.h5', by_name=True)\n",
    "base_net_2D.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      " 576/8000 [=>............................] - ETA: 1s - loss: 3.0328 - acc: 0.5972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phoen\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 2s 272us/step - loss: 0.4791 - acc: 0.8830 - val_loss: 0.0431 - val_acc: 0.9865\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.0345 - acc: 0.9903 - val_loss: 0.0622 - val_acc: 0.9780\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.0265 - val_acc: 0.9895\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 5s 658us/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0278 - val_acc: 0.9895\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 6s 699us/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0448 - val_acc: 0.9855\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 14.5023 - acc: 0.1003 - val_loss: 14.5224 - val_acc: 0.0990\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 14.5023 - acc: 0.1003 - val_loss: 14.5224 - val_acc: 0.0990\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 14.5023 - acc: 0.1003 - val_loss: 14.5224 - val_acc: 0.0990\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 14.5023 - acc: 0.1003 - val_loss: 14.5224 - val_acc: 0.0990\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 14.5023 - acc: 0.1003 - val_loss: 14.5224 - val_acc: 0.0990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b0a0549188>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDX,trainDY,testDX,testDY = Shuffle(X_1_1D,Y_1,split=0.8)\n",
    "model_1D.fit(trainDX,trainDY,batch_size=64,epochs=5,validation_data=[testDX,testDY])\n",
    "trainDX,trainDY,testDX,testDY = Shuffle(X_1_2D,Y_1,split=0.8)\n",
    "model_2D.fit(trainDX,trainDY,batch_size=16,epochs=5,validation_data=[testDX,testDY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1D.save(\"Pretrain-Task1_1D.h5\")\n",
    "model_2D.save(\"Pretrain-Task1_2D.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 1ms/step\n",
      "200/200 [==============================] - 0s 115us/step\n",
      "200/200 [==============================] - 0s 444us/step\n",
      "200/200 [==============================] - 0s 120us/step\n",
      "200/200 [==============================] - 0s 309us/step\n",
      "200/200 [==============================] - 0s 113us/step\n",
      "200/200 [==============================] - 0s 2ms/step\n",
      "200/200 [==============================] - 0s 110us/step\n",
      "200/200 [==============================] - 0s 115us/step\n",
      "200/200 [==============================] - 0s 115us/step\n",
      "Parameter-based model on 1D data: Mean acc: 99.55% Prediction variance: 0.00%\n",
      "200/200 [==============================] - 7s 37ms/step\n",
      "200/200 [==============================] - 0s 863us/step\n",
      "200/200 [==============================] - 0s 578us/step\n",
      "200/200 [==============================] - 0s 425us/step\n",
      "200/200 [==============================] - 0s 503us/step\n",
      "200/200 [==============================] - 0s 648us/step\n",
      "200/200 [==============================] - 0s 818us/step\n",
      "200/200 [==============================] - 0s 502us/step\n",
      "200/200 [==============================] - 0s 594us/step\n",
      "200/200 [==============================] - 0s 888us/step\n",
      "Parameter-based model on 2D data: Mean acc: 9.20% Prediction variance: 0.06%\n"
     ]
    }
   ],
   "source": [
    "model=load_model(\"Pretrain-Task1_1D.h5\")\n",
    "trainDX,trainDY,testDX,testDY = Shuffle(X_1_1D,Y_1,split=0.8)\n",
    "def test_model(model,testDX,testDY):\n",
    "    acc=np.zeros(10,)\n",
    "    for i in range(10):\n",
    "        acc[i]=model.evaluate(testDX[0+(len(testDX)//10)*i:(len(testDX)//10)+(len(testDX)//10)*i],testDY[0+(len(testDX)//10)*i:(len(testDX)//10)+(len(testDX)//10)*i])[1]\n",
    "    print(\"Parameter-based model on 1D data: Mean acc: %.2f\"%(np.mean(acc)*100)+\"%\",\"Prediction variance: %.2f\"%(np.var(acc)*100)+\"%\")\n",
    "\n",
    "test_model(model,testDX,testDY)\n",
    "\n",
    "model=load_model(\"Pretrain-Task1_2D.h5\")\n",
    "trainDX,trainDY,testDX,testDY = Shuffle(X_1_2D,Y_1,split=0.8)\n",
    "def test_model(model,testDX,testDY):\n",
    "    acc=np.zeros(10,)\n",
    "    for i in range(10):\n",
    "        acc[i]=model.evaluate(testDX[0+(len(testDX)//10)*i:(len(testDX)//10)+(len(testDX)//10)*i],testDY[0+(len(testDX)//10)*i:(len(testDX)//10)+(len(testDX)//10)*i])[1]\n",
    "    print(\"Parameter-based model on 2D data: Mean acc: %.2f\"%(np.mean(acc)*100)+\"%\",\"Prediction variance: %.2f\"%(np.var(acc)*100)+\"%\")\n",
    "\n",
    "test_model(model,testDX,testDY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.458px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
