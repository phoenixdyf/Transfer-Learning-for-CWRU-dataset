{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep transfer learning based on MMD (Maximum Mean Discrepency)**\n",
    "---\n",
    "<font color=black size=3 face=雅黑>**Envirionment**: torch = 1.2.0 </font>\n",
    "\n",
    "<font color=black size=3 face=雅黑>**A schematic introduction of implementation proposed feature-based models**</font>\n",
    "\n",
    "<font color=black size=2 face=雅黑>In what follows, we explain how to implement a feature-based (MMD-CNN) in Pytorch for DTL-IFD\n",
    "\n",
    "Schematically, the designed MMD-CNN looks like this:\n",
    "\n",
    "* A `SharedNet` network maps vectors of shape `1024x1(64x64x3)` to extracted feature.\n",
    "    \n",
    "* A `Classifier` network maps source domain CNN feature to a Probability output with ten categories`(10,)`.\n",
    "    \n",
    "* A `rbf_MMD` function calculates the MMD loss between the source and target feature extracted from `SharedNet`.\n",
    "\n",
    "    \n",
    "*Step 1* : We first fed both source domain and target domain data 1D_CNN (2D_CNN) into the `SharedNet`, thus two types of features are \n",
    "extracted as `Source feature` and `Target feature`.\n",
    "\n",
    "*Step 2* : We calculate the **MMD-Loss** between the `Source feature` and `Target feature`, the rbf-mmd-loss is defined as:\n",
    "    $\\operatorname{MMD}[\\mathcal{F}, X, Y]=\\left[\\frac{1}{m^{2}} \\sum_{i, j=1}^{m} k\\left(x_{i}, x_{j}\\right)-\\frac{2}{m n} \\sum_{i, j=1}^{m, n} k\\left(x_{i}, y_{j}\\right)+\\frac{1}{n^{2}} \\sum_{i, j=1}^{n} k\\left(y_{i}, y_{j}\\right)\\right]^{\\frac{1}{2}}$\n",
    "\n",
    "*Step 3* : We calculate the **Classification-loss** only on the `Source feature`.\n",
    "    \n",
    "*Step 4* : We combine the **MMD-Loss** and **Classification-loss** together as **total-loss** and use this **total-loss** for backpropagation and optimize the `SharedNet` and `Classifier`.\n",
    "\n",
    "**By iteratively conducting step 1 to step 4, the domain shift could be covered and the classfication task from source domain to target domain could be implemented** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and make the data-loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import math\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 1D raw signal\n",
    "X_0_1D = np.load(\"./DE/X_0.npy\").reshape((-1,1,1024))\n",
    "X_1_1D = np.load(\"./DE/X_1.npy\").reshape((-1,1,1024))\n",
    "X_2_1D = np.load(\"./DE/X_2.npy\").reshape((-1,1,1024))\n",
    "X_3_1D = np.load(\"./DE/X_3.npy\").reshape((-1,1,1024))\n",
    "# Load 2D pre-processed frequenct spectrum\n",
    "X_0_2D = np.load(\"./DE/Case1_FFT.npy\").reshape((-1,3,64,64))\n",
    "X_1_2D = np.load(\"./DE/Case2_FFT.npy\").reshape((-1,3,64,64))\n",
    "X_2_2D = np.load(\"./DE/Case3_FFT.npy\").reshape((-1,3,64,64))\n",
    "X_3_2D = np.load(\"./DE/Case4_FFT.npy\").reshape((-1,3,64,64))\n",
    "Y_0 = np.load(\"./DE/Y_0.npy\")\n",
    "Y_1 = np.load(\"./DE/Y_1.npy\")\n",
    "Y_2 = np.load(\"./DE/Y_2.npy\")\n",
    "Y_3 = np.load(\"./DE/Y_3.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defination of MMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defination of MMD (Maximum Mean Discrepency) and kernel function**\n",
    "The basic MMD is defined as:\n",
    "$M M D(\\boldsymbol{X}, \\boldsymbol{Y})=\\left\\|\\sum_{i=1}^{n_{1}} \\phi\\left(x_{i}\\right)-\\sum_{j=1}^{n_{2}} \\phi\\left(y_{i}\\right)\\right\\|_{\\mathcal{H}}^{2}$\n",
    "\n",
    "where the kernel function $\\phi(x): \\mathcal{X} \\rightarrow \\mathcal{H}$ indicates the mapping relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     28
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "    total = torch.cat([source, target], dim=0)\n",
    "    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "    L2_distance = ((total0-total1)**2).sum(2)\n",
    "    if fix_sigma:\n",
    "        bandwidth = fix_sigma\n",
    "    else:\n",
    "        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "    bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "    return sum(kernel_val)#/len(kernel_val)\n",
    "\n",
    "def mmd_rbf_accelerate(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    batch_size = int(source.size()[0])\n",
    "    kernels = guassian_kernel(source, target,\n",
    "        kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        s1, s2 = i, (i+1)%batch_size\n",
    "        t1, t2 = s1+batch_size, s2+batch_size\n",
    "        loss += kernels[s1, s2] + kernels[t1, t2]\n",
    "        loss -= kernels[s1, t2] + kernels[s2, t1]\n",
    "    return loss / float(batch_size)\n",
    "def mmd_rbf_noaccelerate(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    batch_size = int(source.size()[0])\n",
    "    kernels = guassian_kernel(source, target,\n",
    "                              kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "    XX = kernels[:batch_size, :batch_size]\n",
    "    YY = kernels[batch_size:, batch_size:]\n",
    "    XY = kernels[:batch_size, batch_size:]\n",
    "    YX = kernels[batch_size:, :batch_size]\n",
    "    loss = torch.mean(XX + YY - XY -YX)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defination of the MMD-CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defination MMD-CNN model for 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BaseNet_2D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel=1, out_channel=10):\n",
    "        super(BaseNet_2D, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=5, stride=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.Laplace(x)\n",
    "        #self.featuremap_Laplace = x.detach() # 核心代码\n",
    "        x = self.conv1(x)\n",
    "        self.featuremap_conv1 = x.detach() # 核心代码\n",
    "        x = self.conv2(x)\n",
    "        self.featuremap_conv2 = x.detach() # 核心代码\n",
    "        x = self.conv3(x)\n",
    "        self.featuremap_conv3 = x.detach() # 核心代码\n",
    "        x = self.conv4(x)\n",
    "        self.featuremap_conv4 = x.detach() # 核心代码\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "    \n",
    "class CNN_2D(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN_2D, self).__init__()\n",
    "        self.sharedNet = BaseNet_2D()\n",
    "        self.fc1 = nn.Linear(256 *11*11,1024)\n",
    "        self.cls_fc = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        loss = 0\n",
    "        source = self.sharedNet(source)  \n",
    "        if self.training == True:\n",
    "            target = self.sharedNet(target)      \n",
    "            #loss += mmd.mmd_rbf_accelerate(source, target)\n",
    "            loss += mmd_rbf_noaccelerate(source, target)\n",
    "        source = self.fc1(source)     \n",
    "        source = self.cls_fc(source)\n",
    "        #target = self.cls_fc(target)\n",
    "\n",
    "        return source, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defination MMD-CNN model for 1D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import pi\n",
    "def Laplace(p):\n",
    "    A = 0.08\n",
    "    ep = 0.03\n",
    "    tal = 0.1\n",
    "    f = 50\n",
    "    w = 2 * pi * f\n",
    "    q = torch.tensor(1 - pow(ep, 2))\n",
    "    y = A * torch.exp((-ep / (torch.sqrt(q))) * (w * (p - tal))) * (-torch.sin(w * (p - tal)))\n",
    "    return y\n",
    "\n",
    "\n",
    "class Laplace_fast(nn.Module):\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, in_channels=1):\n",
    "\n",
    "        super(Laplace_fast, self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "\n",
    "            msg = \"MexhConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size - 1\n",
    "\n",
    "        if kernel_size % 2 == 0:\n",
    "            self.kernel_size = self.kernel_size + 1\n",
    "\n",
    "        self.a_ = nn.Parameter(torch.linspace(1, 10, out_channels)).view(-1, 1)\n",
    "\n",
    "        self.b_ = nn.Parameter(torch.linspace(0, 10, out_channels)).view(-1, 1)\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "\n",
    "        time_disc = torch.linspace(0, 1, steps=int((self.kernel_size)))\n",
    "\n",
    "        p1 = time_disc.cuda() - self.b_.cuda() / self.a_.cuda()\n",
    "\n",
    "        laplace_filter = Laplace(p1)\n",
    "\n",
    "        self.filters = (laplace_filter).view(self.out_channels, 1, self.kernel_size).cuda()\n",
    "\n",
    "\n",
    "        return F.conv1d(waveforms, self.filters, stride=1, padding=1, dilation=1, bias=None, groups=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseNet_1D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel=1, out_channel=10):\n",
    "        super(BaseNet_1D, self).__init__()\n",
    "        self.Laplace = nn.Sequential(\n",
    "            Laplace_fast(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=2))\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Laplace(x)\n",
    "        self.featuremap_Laplace = x.detach() # 核心代码\n",
    "        x = self.conv1(x)\n",
    "        self.featuremap_conv1 = x.detach() # 核心代码\n",
    "        x = self.conv2(x)\n",
    "        self.featuremap_conv2 = x.detach() # 核心代码\n",
    "        x = self.conv3(x)\n",
    "        self.featuremap_conv3 = x.detach() # 核心代码\n",
    "        x = self.conv4(x)\n",
    "        self.featuremap_conv4 = x.detach() # 核心代码\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "    \n",
    "class CNN_1D(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.sharedNet = BaseNet_1D()\n",
    "        self.fc1 = nn.Linear(256 * 120,1024)\n",
    "        self.cls_fc = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        loss = 0\n",
    "        source = self.sharedNet(source)  \n",
    "        if self.training == True:\n",
    "            target = self.sharedNet(target)      \n",
    "            #loss += mmd.mmd_rbf_accelerate(source, target)\n",
    "            loss += mmd_rbf_noaccelerate(source, target)\n",
    "        source = self.fc1(source)     \n",
    "        source = self.cls_fc(source)\n",
    "        #target = self.cls_fc(target)\n",
    "\n",
    "        return source, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defination of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    src_iter = iter(src_train_loader)\n",
    "    tgt_iter = iter(tgt_train_loader)\n",
    "    correct = 0\n",
    "    best_model_wts=copy.deepcopy(model.state_dict())\n",
    "    final_model_wts=copy.deepcopy(model.state_dict())\n",
    "    for i in range(1, iteration+1):\n",
    "        model.train()\n",
    "        LEARNING_RATE = lr / math.pow((1 + 10 * (i - 1) / (iteration)), 0.75)\n",
    "        if (i-1)%100==0:\n",
    "            print('learning rate{: .4f}'.format(LEARNING_RATE) )\n",
    "        optimizer = torch.optim.SGD([\n",
    "        {'params': model.sharedNet.parameters()},\n",
    "        {'params': model.cls_fc.parameters(), 'lr': LEARNING_RATE},\n",
    "        ], lr=LEARNING_RATE / 10, momentum=momentum, weight_decay=l2_decay)\n",
    "        try:\n",
    "            src_data, src_label = src_iter.next()\n",
    "        except Exception as err:\n",
    "            src_iter=iter(src_train_loader)\n",
    "            src_data, src_label = src_iter.next()\n",
    "            \n",
    "        try:\n",
    "            tgt_data, _ = tgt_iter.next()\n",
    "        except Exception as err:\n",
    "            tgt_iter=iter(tgt_train_loader)\n",
    "            tgt_data, _ = tgt_iter.next()\n",
    "            \n",
    "        if cuda:\n",
    "            src_data, src_label = src_data.cuda(), src_label.cuda()\n",
    "            tgt_data = tgt_data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        src_pred, mmd_loss = model(src_data, tgt_data)\n",
    "        cls_loss = F.nll_loss(F.log_softmax(src_pred, dim=1), src_label.long())\n",
    "        lambd = 0.1/ (1 + math.exp(-10 * (i) / iteration)) - 1\n",
    "        loss = cls_loss + lambd * (mmd_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % log_interval == 0:\n",
    "            print('Train iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\tsoft_Loss: {:.6f}\\tmmd_Loss: {:.6f}'.format(\n",
    "                i, 100. * i / iteration, loss.item(), cls_loss.item(), mmd_loss.item()))\n",
    "\n",
    "        if i%(log_interval*20)==0:\n",
    "            t_correct = test(model)\n",
    "            if t_correct > correct:\n",
    "                correct = t_correct\n",
    "                best_model_wts=copy.deepcopy(model.state_dict())\n",
    "            print('src: {} to tgt: {} max correct: {} max accuracy{: .2f}%\\n'.format(\n",
    "              src_name, tgt_name, correct, 100. * correct / tgt_dataset_len ))\n",
    "        #if i>=1999:\n",
    "            #final_model_wts=copy.deepcopy(model.state_dict())\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defination of the testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     16
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for tgt_test_data, tgt_test_label in tgt_test_loader:\n",
    "            if cuda:\n",
    "                tgt_test_data, tgt_test_label = tgt_test_data.cuda(), tgt_test_label.cuda()\n",
    "                tgt_test_data, tgt_test_label = Variable(tgt_test_data), Variable(tgt_test_label)\n",
    "                tgt_pred, mmd_loss = model(tgt_test_data, tgt_test_data)\n",
    "                test_loss += F.nll_loss(F.log_softmax(tgt_pred, dim = 1), tgt_test_label.long(), reduction='sum').item() # sum up batch loss\n",
    "                pred = tgt_pred.data.max(1)[1] # get the index of the max log-probability\n",
    "                correct += pred.eq(tgt_test_label.long().data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= tgt_dataset_len\n",
    "    print('\\n{} set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        tgt_name, test_loss, correct, tgt_dataset_len,\n",
    "        100. * correct / tgt_dataset_len))\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the 1D  data into tensor\n",
    "X_0_1D=torch.from_numpy(X_0_1D.astype(np.float32))\n",
    "\n",
    "X_1_1D=torch.from_numpy(X_1_1D.astype(np.float32))\n",
    "\n",
    "X_2_1D=torch.from_numpy(X_2_1D.astype(np.float32))\n",
    "\n",
    "X_3_1D=torch.from_numpy(X_3_1D.astype(np.float32))\n",
    "\n",
    "#Transform the 2D  data into tensor\n",
    "X_0_2D=torch.from_numpy(X_0_2D.astype(np.float32))\n",
    "\n",
    "X_1_2D=torch.from_numpy(X_1_2D.astype(np.float32))\n",
    "\n",
    "X_2_2D=torch.from_numpy(X_2_2D.astype(np.float32))\n",
    "\n",
    "X_3_2D=torch.from_numpy(X_3_2D.astype(np.float32))\n",
    "\n",
    "#Transform the  label into tensor\n",
    "Y_0=torch.from_numpy(Y_0.astype(np.float32))\n",
    "\n",
    "Y_1=torch.from_numpy(Y_1.astype(np.float32))\n",
    "\n",
    "Y_2=torch.from_numpy(Y_2.astype(np.float32))\n",
    "\n",
    "Y_3=torch.from_numpy(Y_3.astype(np.float32))\n",
    "\n",
    "#Combine the 1D-X and Y with TensorDataset\n",
    "data_0_1D=Data.TensorDataset(X_0_1D,Y_0)\n",
    "data_1_1D=Data.TensorDataset(X_1_1D,Y_1)\n",
    "data_2_1D=Data.TensorDataset(X_2_1D,Y_2)\n",
    "data_3_1D=Data.TensorDataset(X_3_1D,Y_3)\n",
    "\n",
    "#Combine the 2D-X and Y with TensorDataset\n",
    "data_0_2D=Data.TensorDataset(X_0_2D,Y_0)\n",
    "data_1_2D=Data.TensorDataset(X_1_2D,Y_1)\n",
    "data_2_2D=Data.TensorDataset(X_2_2D,Y_2)\n",
    "data_3_2D=Data.TensorDataset(X_3_2D,Y_3)\n",
    "\n",
    "\n",
    "def Split(full_dataset):\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# 1D training data and testing data \n",
    "Train_0_1D,Test_0_1D = Split(data_0_1D)\n",
    "Train_1_1D,Test_1_1D = Split(data_1_1D)\n",
    "Train_2_1D,Test_2_1D = Split(data_2_1D)\n",
    "Train_3_1D,Test_3_1D = Split(data_3_1D)\n",
    "\n",
    "# 2D training data and testing data \n",
    "Train_0_2D,Test_0_2D = Split(data_0_2D)\n",
    "Train_1_2D,Test_1_2D = Split(data_1_2D)\n",
    "Train_2_2D,Test_2_2D = Split(data_2_2D)\n",
    "Train_3_2D,Test_3_2D = Split(data_3_2D)\n",
    "#Defination of data loader\n",
    "src_train_loader=Data.DataLoader(dataset=Train_0_2D,batch_size=16,shuffle=True,num_workers=0)\n",
    "tgt_train_loader=Data.DataLoader(dataset=Train_1_2D,batch_size=16,shuffle=True,num_workers=0)\n",
    "src_test_loader=Data.DataLoader(dataset=Test_0_2D,batch_size=16,shuffle=True,num_workers=0)\n",
    "tgt_test_loader=Data.DataLoader(dataset=Test_1_2D,batch_size=16,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_2D(\n",
      "  (sharedNet): BaseNet_2D(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv4): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=5, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=30976, out_features=1024, bias=True)\n",
      "  (cls_fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n",
      "learning rate 0.0010\n",
      "Train iter: 10 [(1%)]\tLoss: 2.463044\tsoft_Loss: 2.892381\tmmd_Loss: 0.453125\n",
      "Train iter: 20 [(2%)]\tLoss: 1.432707\tsoft_Loss: 1.915583\tmmd_Loss: 0.510970\n",
      "Train iter: 30 [(3%)]\tLoss: 1.530531\tsoft_Loss: 1.961538\tmmd_Loss: 0.457274\n",
      "Train iter: 40 [(4%)]\tLoss: 1.486051\tsoft_Loss: 1.827640\tmmd_Loss: 0.363341\n",
      "Train iter: 50 [(5%)]\tLoss: 1.320829\tsoft_Loss: 1.809399\tmmd_Loss: 0.521000\n",
      "Train iter: 60 [(6%)]\tLoss: 0.858744\tsoft_Loss: 1.398108\tmmd_Loss: 0.576592\n",
      "Train iter: 70 [(7%)]\tLoss: 1.273446\tsoft_Loss: 1.625966\tmmd_Loss: 0.377761\n",
      "Train iter: 80 [(8%)]\tLoss: 0.937937\tsoft_Loss: 1.330041\tmmd_Loss: 0.421162\n",
      "Train iter: 90 [(9%)]\tLoss: 0.587899\tsoft_Loss: 1.000063\tmmd_Loss: 0.443709\n",
      "Train iter: 100 [(10%)]\tLoss: 0.756212\tsoft_Loss: 1.305077\tmmd_Loss: 0.592155\n",
      "learning rate 0.0006\n",
      "Train iter: 110 [(11%)]\tLoss: 0.650842\tsoft_Loss: 1.103175\tmmd_Loss: 0.489022\n",
      "Train iter: 120 [(12%)]\tLoss: 0.190513\tsoft_Loss: 0.772506\tmmd_Loss: 0.630445\n",
      "Train iter: 130 [(13%)]\tLoss: 0.585121\tsoft_Loss: 0.933548\tmmd_Loss: 0.378143\n",
      "Train iter: 140 [(14%)]\tLoss: 0.555716\tsoft_Loss: 1.006139\tmmd_Loss: 0.489707\n",
      "Train iter: 150 [(15%)]\tLoss: 0.479791\tsoft_Loss: 0.815134\tmmd_Loss: 0.365201\n",
      "Train iter: 160 [(16%)]\tLoss: 0.233149\tsoft_Loss: 0.846654\tmmd_Loss: 0.669182\n",
      "Train iter: 170 [(17%)]\tLoss: 0.334596\tsoft_Loss: 0.840687\tmmd_Loss: 0.552835\n",
      "Train iter: 180 [(18%)]\tLoss: 0.260018\tsoft_Loss: 0.724013\tmmd_Loss: 0.507551\n",
      "Train iter: 190 [(19%)]\tLoss: 0.317622\tsoft_Loss: 0.603883\tmmd_Loss: 0.313536\n",
      "Train iter: 200 [(20%)]\tLoss: 0.385900\tsoft_Loss: 0.692593\tmmd_Loss: 0.336316\n",
      "\n",
      "b set: Average loss: 0.8573, Accuracy: 1837/2000 (91.00%)\n",
      "\n",
      "src: a to tgt: b max correct: 1837 max accuracy 91.00%\n",
      "\n",
      "learning rate 0.0004\n",
      "Train iter: 210 [(21%)]\tLoss: -0.108665\tsoft_Loss: 0.409155\tmmd_Loss: 0.568464\n",
      "Train iter: 220 [(22%)]\tLoss: 0.192304\tsoft_Loss: 0.590006\tmmd_Loss: 0.437047\n",
      "Train iter: 230 [(23%)]\tLoss: 0.095942\tsoft_Loss: 0.598890\tmmd_Loss: 0.553231\n",
      "Train iter: 240 [(24%)]\tLoss: 0.059682\tsoft_Loss: 0.485447\tmmd_Loss: 0.468740\n",
      "Train iter: 250 [(25%)]\tLoss: 0.339626\tsoft_Loss: 0.620497\tmmd_Loss: 0.309470\n",
      "Train iter: 260 [(26%)]\tLoss: 0.175192\tsoft_Loss: 0.616790\tmmd_Loss: 0.486924\n",
      "Train iter: 270 [(27%)]\tLoss: 0.010858\tsoft_Loss: 0.423820\tmmd_Loss: 0.455658\n",
      "Train iter: 280 [(28%)]\tLoss: 0.022095\tsoft_Loss: 0.399445\tmmd_Loss: 0.416625\n",
      "Train iter: 290 [(29%)]\tLoss: -0.024550\tsoft_Loss: 0.416090\tmmd_Loss: 0.486780\n",
      "Train iter: 300 [(30%)]\tLoss: 0.182855\tsoft_Loss: 0.509968\tmmd_Loss: 0.361555\n",
      "learning rate 0.0004\n",
      "Train iter: 310 [(31%)]\tLoss: -0.202076\tsoft_Loss: 0.519019\tmmd_Loss: 0.797397\n",
      "Train iter: 320 [(32%)]\tLoss: 0.060937\tsoft_Loss: 0.405491\tmmd_Loss: 0.381180\n",
      "Train iter: 330 [(33%)]\tLoss: -0.015157\tsoft_Loss: 0.396934\tmmd_Loss: 0.456076\n",
      "Train iter: 340 [(34%)]\tLoss: 0.154617\tsoft_Loss: 0.450070\tmmd_Loss: 0.327108\n",
      "Train iter: 350 [(35%)]\tLoss: -0.199083\tsoft_Loss: 0.349646\tmmd_Loss: 0.607720\n",
      "Train iter: 360 [(36%)]\tLoss: -0.172625\tsoft_Loss: 0.345298\tmmd_Loss: 0.573775\n",
      "Train iter: 370 [(37%)]\tLoss: -0.041406\tsoft_Loss: 0.259597\tmmd_Loss: 0.333553\n",
      "Train iter: 380 [(38%)]\tLoss: -0.348911\tsoft_Loss: 0.310401\tmmd_Loss: 0.730792\n",
      "Train iter: 390 [(39%)]\tLoss: -0.288121\tsoft_Loss: 0.425865\tmmd_Loss: 0.791573\n",
      "Train iter: 400 [(40%)]\tLoss: -0.476112\tsoft_Loss: 0.174383\tmmd_Loss: 0.721331\n",
      "\n",
      "b set: Average loss: 0.5244, Accuracy: 1840/2000 (92.00%)\n",
      "\n",
      "src: a to tgt: b max correct: 1840 max accuracy 92.00%\n",
      "\n",
      "learning rate 0.0003\n",
      "Train iter: 410 [(41%)]\tLoss: -0.150514\tsoft_Loss: 0.269289\tmmd_Loss: 0.465604\n",
      "Train iter: 420 [(42%)]\tLoss: -0.012447\tsoft_Loss: 0.260524\tmmd_Loss: 0.302804\n",
      "Train iter: 430 [(43%)]\tLoss: -0.052077\tsoft_Loss: 0.301833\tmmd_Loss: 0.392649\n",
      "Train iter: 440 [(44%)]\tLoss: -0.267628\tsoft_Loss: 0.264086\tmmd_Loss: 0.589998\n",
      "Train iter: 450 [(45%)]\tLoss: -0.118979\tsoft_Loss: 0.324901\tmmd_Loss: 0.492599\n",
      "Train iter: 460 [(46%)]\tLoss: -0.073248\tsoft_Loss: 0.236201\tmmd_Loss: 0.343453\n",
      "Train iter: 470 [(47%)]\tLoss: -0.116710\tsoft_Loss: 0.203614\tmmd_Loss: 0.355559\n",
      "Train iter: 480 [(48%)]\tLoss: -0.155999\tsoft_Loss: 0.157146\tmmd_Loss: 0.347624\n",
      "Train iter: 490 [(49%)]\tLoss: -0.259476\tsoft_Loss: 0.234792\tmmd_Loss: 0.548736\n",
      "Train iter: 500 [(50%)]\tLoss: -0.181152\tsoft_Loss: 0.194907\tmmd_Loss: 0.417532\n",
      "learning rate 0.0003\n",
      "Train iter: 510 [(51%)]\tLoss: -0.313719\tsoft_Loss: 0.245532\tmmd_Loss: 0.620972\n",
      "Train iter: 520 [(52%)]\tLoss: -0.107440\tsoft_Loss: 0.254045\tmmd_Loss: 0.401405\n",
      "Train iter: 530 [(53%)]\tLoss: -0.144096\tsoft_Loss: 0.242986\tmmd_Loss: 0.429854\n",
      "Train iter: 540 [(54%)]\tLoss: -0.375219\tsoft_Loss: 0.235504\tmmd_Loss: 0.678242\n",
      "Train iter: 550 [(55%)]\tLoss: -0.157272\tsoft_Loss: 0.144565\tmmd_Loss: 0.335223\n",
      "Train iter: 560 [(56%)]\tLoss: -0.217364\tsoft_Loss: 0.219706\tmmd_Loss: 0.485435\n",
      "Train iter: 570 [(57%)]\tLoss: -0.141392\tsoft_Loss: 0.231742\tmmd_Loss: 0.414439\n",
      "Train iter: 580 [(58%)]\tLoss: -0.173403\tsoft_Loss: 0.243514\tmmd_Loss: 0.463085\n",
      "Train iter: 590 [(59%)]\tLoss: -0.200399\tsoft_Loss: 0.190104\tmmd_Loss: 0.433761\n",
      "Train iter: 600 [(60%)]\tLoss: -0.309188\tsoft_Loss: 0.108410\tmmd_Loss: 0.463870\n",
      "\n",
      "b set: Average loss: 0.3964, Accuracy: 1886/2000 (94.00%)\n",
      "\n",
      "src: a to tgt: b max correct: 1886 max accuracy 94.00%\n",
      "\n",
      "learning rate 0.0002\n",
      "Train iter: 610 [(61%)]\tLoss: -0.204309\tsoft_Loss: 0.175776\tmmd_Loss: 0.422212\n",
      "Train iter: 620 [(62%)]\tLoss: -0.155319\tsoft_Loss: 0.215195\tmmd_Loss: 0.411589\n",
      "Train iter: 630 [(63%)]\tLoss: -0.248499\tsoft_Loss: 0.197570\tmmd_Loss: 0.495531\n",
      "Train iter: 640 [(64%)]\tLoss: -0.100558\tsoft_Loss: 0.191094\tmmd_Loss: 0.323998\n",
      "Train iter: 650 [(65%)]\tLoss: -0.285455\tsoft_Loss: 0.205813\tmmd_Loss: 0.545762\n",
      "Train iter: 660 [(66%)]\tLoss: -0.127991\tsoft_Loss: 0.212818\tmmd_Loss: 0.378620\n",
      "Train iter: 670 [(67%)]\tLoss: -0.240458\tsoft_Loss: 0.147311\tmmd_Loss: 0.430796\n",
      "Train iter: 680 [(68%)]\tLoss: -0.151941\tsoft_Loss: 0.158374\tmmd_Loss: 0.344752\n",
      "Train iter: 690 [(69%)]\tLoss: -0.077518\tsoft_Loss: 0.208116\tmmd_Loss: 0.317336\n",
      "Train iter: 700 [(70%)]\tLoss: -0.153862\tsoft_Loss: 0.167321\tmmd_Loss: 0.356834\n",
      "learning rate 0.0002\n",
      "Train iter: 710 [(71%)]\tLoss: -0.310589\tsoft_Loss: 0.128739\tmmd_Loss: 0.488097\n",
      "Train iter: 720 [(72%)]\tLoss: -0.238187\tsoft_Loss: 0.299352\tmmd_Loss: 0.597215\n",
      "Train iter: 730 [(73%)]\tLoss: -0.234502\tsoft_Loss: 0.227652\tmmd_Loss: 0.513466\n",
      "Train iter: 740 [(74%)]\tLoss: -0.139423\tsoft_Loss: 0.134011\tmmd_Loss: 0.303795\n",
      "Train iter: 750 [(75%)]\tLoss: -0.094962\tsoft_Loss: 0.139568\tmmd_Loss: 0.260573\n",
      "Train iter: 760 [(76%)]\tLoss: -0.180338\tsoft_Loss: 0.133170\tmmd_Loss: 0.348323\n",
      "Train iter: 770 [(77%)]\tLoss: -0.053343\tsoft_Loss: 0.204453\tmmd_Loss: 0.286425\n",
      "Train iter: 780 [(78%)]\tLoss: -0.134944\tsoft_Loss: 0.162166\tmmd_Loss: 0.330107\n",
      "Train iter: 790 [(79%)]\tLoss: -0.365227\tsoft_Loss: 0.129077\tmmd_Loss: 0.549204\n",
      "Train iter: 800 [(80%)]\tLoss: -0.106921\tsoft_Loss: 0.179098\tmmd_Loss: 0.317787\n",
      "\n",
      "b set: Average loss: 0.3281, Accuracy: 1930/2000 (96.00%)\n",
      "\n",
      "src: a to tgt: b max correct: 1930 max accuracy 96.00%\n",
      "\n",
      "learning rate 0.0002\n",
      "Train iter: 810 [(81%)]\tLoss: -0.336760\tsoft_Loss: 0.136297\tmmd_Loss: 0.525602\n",
      "Train iter: 820 [(82%)]\tLoss: -0.116754\tsoft_Loss: 0.141867\tmmd_Loss: 0.287348\n",
      "Train iter: 830 [(83%)]\tLoss: -0.120202\tsoft_Loss: 0.213693\tmmd_Loss: 0.370984\n",
      "Train iter: 840 [(84%)]\tLoss: -0.126625\tsoft_Loss: 0.166877\tmmd_Loss: 0.326105\n",
      "Train iter: 850 [(85%)]\tLoss: -0.172083\tsoft_Loss: 0.136538\tmmd_Loss: 0.342905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter: 860 [(86%)]\tLoss: -0.123386\tsoft_Loss: 0.199539\tmmd_Loss: 0.358799\n",
      "Train iter: 870 [(87%)]\tLoss: -0.363580\tsoft_Loss: 0.148151\tmmd_Loss: 0.568579\n",
      "Train iter: 880 [(88%)]\tLoss: -0.576198\tsoft_Loss: 0.090255\tmmd_Loss: 0.740492\n",
      "Train iter: 890 [(89%)]\tLoss: -0.572154\tsoft_Loss: 0.091364\tmmd_Loss: 0.737231\n",
      "Train iter: 900 [(90%)]\tLoss: -0.269056\tsoft_Loss: 0.154372\tmmd_Loss: 0.470469\n",
      "learning rate 0.0002\n",
      "Train iter: 910 [(91%)]\tLoss: -0.524307\tsoft_Loss: 0.126250\tmmd_Loss: 0.722832\n",
      "Train iter: 920 [(92%)]\tLoss: -0.233430\tsoft_Loss: 0.094341\tmmd_Loss: 0.364186\n",
      "Train iter: 930 [(93%)]\tLoss: -0.418766\tsoft_Loss: 0.148872\tmmd_Loss: 0.630702\n",
      "Train iter: 940 [(94%)]\tLoss: -0.191008\tsoft_Loss: 0.094864\tmmd_Loss: 0.317633\n",
      "Train iter: 950 [(95%)]\tLoss: -0.399662\tsoft_Loss: 0.076095\tmmd_Loss: 0.528615\n",
      "Train iter: 960 [(96%)]\tLoss: -0.463621\tsoft_Loss: 0.098067\tmmd_Loss: 0.624094\n",
      "Train iter: 970 [(97%)]\tLoss: -0.160582\tsoft_Loss: 0.113562\tmmd_Loss: 0.304603\n",
      "Train iter: 980 [(98%)]\tLoss: -0.306709\tsoft_Loss: 0.101213\tmmd_Loss: 0.453244\n",
      "Train iter: 990 [(99%)]\tLoss: -0.306152\tsoft_Loss: 0.125507\tmmd_Loss: 0.479618\n",
      "Train iter: 1000 [(100%)]\tLoss: -0.289471\tsoft_Loss: 0.139051\tmmd_Loss: 0.476133\n",
      "\n",
      "b set: Average loss: 0.2870, Accuracy: 1936/2000 (96.00%)\n",
      "\n",
      "src: a to tgt: b max correct: 1936 max accuracy 96.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training settings\n",
    "batch_size =16\n",
    "iteration=1000\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "no_cuda =False\n",
    "seed = 8\n",
    "log_interval = 10\n",
    "l2_decay = 5e-4\n",
    "model = CNN_2D().cuda()\n",
    "print(model)\n",
    "src_dataset_len = len(src_train_loader.dataset)\n",
    "tgt_dataset_len = len(tgt_test_loader.dataset)\n",
    "src_loader_len = len(src_train_loader)\n",
    "tgt_loader_len = len(tgt_train_loader)\n",
    "src_name = \"a\"\n",
    "tgt_name = \"b\"\n",
    "no_cuda =False\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "my_net=train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phoen\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type CNN_2D. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\phoen\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type BaseNet_2D. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "my_net.eval()\n",
    "torch.save(my_net, 'Feature-based-Task1-2D.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model1 = torch.load('Feature-based-Task1-2D.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " meab accuracy 96.80%\n",
      " and mean std  0.34%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Defination of data loader\n",
    "src_train_loader=Data.DataLoader(dataset=Train_0_2D,batch_size=16,shuffle=True,num_workers=0)\n",
    "tgt_train_loader=Data.DataLoader(dataset=Train_1_2D,batch_size=16,shuffle=True,num_workers=0)\n",
    "src_test_loader=Data.DataLoader(dataset=Test_0_2D,batch_size=16,shuffle=True,num_workers=0)\n",
    "tgt_test_loader=Data.DataLoader(dataset=Test_1_2D,batch_size=16,shuffle=True,num_workers=0)\n",
    "model1.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "src_dataset_len = len(src_train_loader.dataset)\n",
    "tgt_dataset_len = len(tgt_test_loader.dataset)\n",
    "src_loader_len = len(src_train_loader)\n",
    "tgt_loader_len = len(tgt_train_loader)\n",
    "total=[]\n",
    "for tgt_test_data, tgt_test_label in tgt_test_loader:\n",
    "    no_cuda =False\n",
    "    cuda = not no_cuda and torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        \n",
    "        tgt_test_data, tgt_test_label = tgt_test_data.cuda(), tgt_test_label.cuda()\n",
    "        tgt_test_data, tgt_test_label = Variable(tgt_test_data), Variable(tgt_test_label)\n",
    "        tgt_pred, mmd_loss = model1(tgt_test_data, tgt_test_data)\n",
    "        test_loss += F.nll_loss(F.log_softmax(tgt_pred, dim = 1), tgt_test_label.long(), reduction='sum').item() # sum up batch loss\n",
    "        pred = tgt_pred.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct_each=pred.eq(tgt_test_label.long().data.view_as(pred)).cpu().sum()/ torch.tensor(tgt_dataset_len//10).float()\n",
    "        correct_each=correct_each.numpy()\n",
    "        total.append(correct_each)\n",
    "        std=np.std(total)\n",
    "        correct += pred.eq(tgt_test_label.long().data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "print(' meab accuracy{: .2f}%\\n and mean std {: .2f}%\\n'.format(100* correct / torch.tensor(tgt_dataset_len).float(),100*std ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310.417px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
